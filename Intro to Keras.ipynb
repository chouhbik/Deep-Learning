{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  What We Are Going To Do:\n",
    "\n",
    "We are going to classify images of handwritten digits (MNIST dataset) using a fully-connected neural network.\n",
    "\n",
    "After successful training, our model will be able to guess the digit in the image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will import all the packages we need\n",
    "\n",
    "Note: You can also import classes/functions within a package directly (e.g. from keras.layers import Dense).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import matplotlib.pyplot as plt #This package is for plotting\n",
    "%matplotlib inline  \n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Data:\n",
    "\n",
    "The dataset is loaded in this section. \n",
    "\n",
    "**1-0. Check the dimensions of data and its minimum & maximum.**\n",
    "\n",
    "Note: You should paste the dataset file in '~/.keras/datasets/' directory or it will start downloading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data dim: (60000, 28, 28)\n",
      "test data dim: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Print x_train and x_test dimensions\n",
    "print('train data dim:', x_train.shape)\n",
    "print('test data dim:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train examples: 60000\n",
      "test examples: 10000\n"
     ]
    }
   ],
   "source": [
    "# How many train/test samples does it have?\n",
    "print('train examples:',len(x_train))\n",
    "print('test examples:',len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train => max : 255 , min : 0\n",
      "x_test => max : 255 , min : 0\n"
     ]
    }
   ],
   "source": [
    "# Print the minimum and maximum of x_train and x_test(use numpy min and max functions)\n",
    "print('x_train => max : {} , min : {}'.format(np.max(x_train),np.min(x_train)))\n",
    "print('x_test => max : {} , min : {}'.format(np.max(x_test),np.min(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1-1. Check a random sample of train data and its label.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# generate a random number. (use numpy random.randint)\n",
    "rand_num = np.random.randint(1,59999)\n",
    "# plot using plt.imshow() & plt.show()\n",
    "plt.imshow(x_train[rand_num],cmap='gray')\n",
    "plt.show()\n",
    "# print its label\n",
    "print(y_train[rand_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1-2. Our Network accept 1D data. So we should flatten our 2D image, then print the dimension of the result arrays.**\n",
    "\n",
    "Use numpy reshape function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape x_train and x_test using numpy reshape function\n",
    "x_train = x_train.reshape(-1,28*28)\n",
    "x_test = x_test.reshape(-1,28*28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1-3. Normalize data by rescaling them to (0,1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize x_train and x_test by dividing them by max of x_train:\n",
    "x_train = x_train/np.max(x_train)\n",
    "x_test = x_test/np.max(x_train)\n",
    "np.max(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1-4. Convert label arrays to 1-hot representation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use keras.utils.to_categorical\n",
    "y_train= keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test= keras.utils.to_categorical(y_test, num_classes=10)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2-0. Add the following layers to the network:**\n",
    "* Hidden Layer 1: Fully Conncted + Relu Activition (e.g. 512 Nuerons)\n",
    "* Hidden Layer 2: Fully Connected + Relu Activition (e.g. 512 Neurons)\n",
    "* Outout Layer: Fully Connected + Softmax Activition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Add the layers to model here.\n",
    "# Hidden Layer1: 512 Neurons+ relu activation + Normal Initialization mean=0, std = 0.1(RandomNormal). You Should define input shape for layer1\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,), kernel_initializer= RandomNormal(0,0.1)))\n",
    "# Hidden Layer2: 512 Neurons+ relu activation + Normal Initialization mean=0, std = 0.1(RandomNormal).\n",
    "model.add(Dense(512, activation='relu', kernel_initializer= RandomNormal(0,0.1)))\n",
    "# Output Layer1: Question: How many neurons?what kind of activation function? + Normal Initialization mean=0, std = 0.1(RandomNormal).\n",
    "model.add(Dense(10, activation='softmax', kernel_initializer= RandomNormal(0,0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2-1. Determine loss function, optimizer and metrics for the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer and its learning rate\n",
    "sgd = SGD(lr=0.01)\n",
    "# Use categorical_crossentropy as loss function, and accuracy as one of the metrics (you may add other metrics too). \n",
    "# use model.compile to add the above parameters to the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2-2. Print the review of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "# Here we saved the raw model without any training. we will use it later.\n",
    "model.save('raw_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train And Evaluate Model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3-0. Train model on training data using model.fit for 3 epoch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/3\n",
      "48000/48000 [==============================] - 8s 160us/step - loss: 0.4096 - accuracy: 0.8763 - val_loss: 0.2471 - val_accuracy: 0.9271\n",
      "Epoch 2/3\n",
      "48000/48000 [==============================] - 8s 162us/step - loss: 0.2160 - accuracy: 0.9373 - val_loss: 0.1966 - val_accuracy: 0.9434\n",
      "Epoch 3/3\n",
      "48000/48000 [==============================] - 8s 157us/step - loss: 0.1672 - accuracy: 0.9515 - val_loss: 0.1765 - val_accuracy: 0.9492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2afc9af9f98>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use model.fit to train the model. Following hyperparameters are suggested (but you may change them if you want):\n",
    "# batch_size = 32\n",
    "# epochs = 3\n",
    "# Validation data percentage: 20%\n",
    "# use verbose for logging the training\n",
    "model.fit(x = x_train, y= y_train, batch_size=32, epochs=3,\n",
    "          verbose=1, callbacks=None, validation_split=0.2, validation_data=None, shuffle=True,\n",
    "          class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3-1. Evaluate model on test data using model.evaluate. Print the model accuracy on test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 41us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[26.552327407788976, 0.9452000260353088]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=x_test, y=y_test, batch_size=None, verbose=1, sample_weight=None, steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3-2. Save model**\n",
    "\n",
    "In Keras, you can save the model to a HDF5 file(.h5) and reload it later simply by model.save(filepath) and keras.models.load_model(filepath), respectively.\n",
    "\n",
    "The saved model contains:\n",
    "* the architecture of the model, allowing to re-create the model\n",
    "* the weights of the model\n",
    "* the training configuration (loss, optimizer)\n",
    "* the state of the optimizer, allowing to resume training exactly where you left off.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model here:\n",
    "model.save('mlp.h5')\n",
    "# Delete model to make sure you reload it correctly:\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3-3. Load model and Predict label for a random image in train set. Verify predicted label by ploting the image.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# reload the model here:\n",
    "model = load_model('mlp.h5')\n",
    "# generate a random number. (use numpy random.randint) and use model.predict to predict its label. \n",
    "rand = np.random.randint(1,60000)\n",
    "img = x_train[rand]\n",
    "img = np.expand_dims(img,axis=0)\n",
    "pre_label= model.predict(img, batch_size=None, verbose=0, steps=None)\n",
    "index = np.argmax(pre_label)\n",
    "print(y_train[rand])\n",
    "print(np.argmax(y_train[rand]))\n",
    "# print its label\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3-4. Continue training + Callbacks**\n",
    "\n",
    "Please Examine how these callbacks work. you may add other callbacks to this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 8s 171us/step - loss: 0.1401 - accuracy: 0.9598 - val_loss: 0.1523 - val_accuracy: 0.9565\n",
      "Epoch 2/10\n",
      "  864/48000 [..............................] - ETA: 9s - loss: 0.1183 - accuracy: 0.9699"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_accuracy,loss,accuracy\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 8s 174us/step - loss: 0.1188 - accuracy: 0.9664 - val_loss: 0.1372 - val_accuracy: 0.9602\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 9s 196us/step - loss: 0.1039 - accuracy: 0.9712 - val_loss: 0.1327 - val_accuracy: 0.9616\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 9s 188us/step - loss: 0.0914 - accuracy: 0.9748 - val_loss: 0.1258 - val_accuracy: 0.9628\n",
      "Epoch 5/10\n",
      "23456/48000 [=============>................] - ETA: 3s - loss: 0.0815 - accuracy: 0.9771"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-47020e84b29a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                     \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                     callbacks = callback)\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    199\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[0mbatch_hook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mbatch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    364\u001b[0m         \"\"\"\n\u001b[0;32m    365\u001b[0m         \u001b[1;31m# For backwards compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    601\u001b[0m         \u001b[1;31m# will be handled by on_epoch_end.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, current, values)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    345\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mimport_lock_held\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m                 \u001b[0mevt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m                 \u001b[1;31m# and give a timeout to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[1;31m# wake event thread (message content is ignored)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    393\u001b[0m                                  copy_threshold=self.copy_threshold)\n\u001b[0;32m    394\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We will use two callbacks here: EarlyStopping, CSVLogger (you may add other callbacks to this list)\n",
    "callback = [keras.callbacks.EarlyStopping(monitor='val_acc', verbose=1, min_delta=0.01, patience = 2, mode = 'max'),\n",
    "            keras.callbacks.CSVLogger('log.csv')]\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size = 32,\n",
    "                    epochs = 10,\n",
    "                    verbose = 1,\n",
    "                    validation_split = 0.2,\n",
    "                    callbacks = callback)\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['loss'])\n",
    "plt.title('loss')\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(history.epoch,history.history['acc'])\n",
    "plt.title('accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**4-0. Initialization is important!**\n",
    "\n",
    "Redefine Network, this time use mean=0 and std=1 for initialization.\n",
    "\n",
    "Then Try to fit the model.\n",
    "\n",
    "Repeat this with other initialization methods and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 324us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[14.573981651306152, 0.095799999999999996]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bad model!!!\n",
    "# import new functions\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.initializers import TruncatedNormal\n",
    "\n",
    "model2 = Sequential()\n",
    "# Add the layers to model here.\n",
    "model2.add(Dense(1024, activation='relu', input_shape=(784,), kernel_initializer= TruncatedNormal(0,1)))\n",
    "# Hidden Layer2: 512 Neurons+ relu activation + Normal Initialization mean=0, std = 0.1(RandomNormal).\n",
    "model2.add(Dense(512, activation='relu', kernel_initializer= TruncatedNormal(0,1)))\n",
    "# Output Layer1: Question: How many neurons?what kind of activation function? + Normal Initialization mean=0, std = 0.1(RandomNormal).\n",
    "model2.add(Dense(10, activation='softmax', kernel_initializer= TruncatedNormal(0,1)))\n",
    "\n",
    "# define the optimizer and its learning rate\n",
    "adagrad = Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "# Use categorical_crossentropy as loss function, and accuracy as one of the metrics (you may add other metrics too). \n",
    "# use model.compile to add the above parameters to the model\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adagrad,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Use model.fit to train the model. Following hyperparameters are suggested (but you may change them if you want):\n",
    "# batch_size = 32\n",
    "# epochs = 3\n",
    "# Validation data percentage: 20%\n",
    "model2.fit(x = x_train, y= y_train, batch_size=32, epochs=3,\n",
    "          verbose=0, callbacks=None, validation_split=0.2, validation_data=None, shuffle=True,\n",
    "          class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n",
    "\n",
    "model2.evaluate(x=x_test, y=y_test, batch_size=None, verbose=1, sample_weight=None, steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4-1. Overfitting/Underfitting**\n",
    "\n",
    "Load the 'raw_model.h5' and this time use 1 percent of training data for training, and all test data for validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 480 samples, validate on 120 samples\n",
      "Epoch 1/3\n",
      "480/480 [==============================] - 1s 1ms/step - loss: 2.0917 - acc: 0.3229 - val_loss: 1.9402 - val_acc: 0.3250\n",
      "Epoch 2/3\n",
      "480/480 [==============================] - 0s 527us/step - loss: 1.1335 - acc: 0.6708 - val_loss: 1.5074 - val_acc: 0.5417\n",
      "Epoch 3/3\n",
      "480/480 [==============================] - 0s 489us/step - loss: 0.7875 - acc: 0.7729 - val_loss: 1.3289 - val_acc: 0.6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.7772133167266846, 0.63639999999999997]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "# Load model here and fit it with less data\n",
    "model = load_model('raw_model.h5')\n",
    "model.fit(x = x_train[0:int((len(x_train)/100)),], y= y_train[0:int((len(x_train)/100)),], batch_size=32, epochs=3, validation_split=0.2)\n",
    "model.evaluate(x=x_test, y=y_test, batch_size=None, verbose=0, sample_weight=None, steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model is underfitted!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create another fully-connected model with 1 hidden layer(5 Neurons). Use all training data for training this model. (10 Training epoch)\n",
    "\n",
    "Observe the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/3\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 2.7115 - acc: 0.2056 - val_loss: 2.1039 - val_acc: 0.2613\n",
      "Epoch 2/3\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 2.0464 - acc: 0.2692 - val_loss: 1.9671 - val_acc: 0.2988\n",
      "Epoch 3/3\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 1.9330 - acc: 0.3025 - val_loss: 1.8683 - val_acc: 0.3339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.5657068298339851, 0.25569999999999998]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Dense(5, activation='relu', input_shape=(784,), kernel_initializer= RandomNormal(0,1)))\n",
    "model3.add(Dense(10, activation='softmax', kernel_initializer= RandomNormal(0,1)))\n",
    "\n",
    "# define the optimizer and its learning rate\n",
    "sgd = SGD(lr=0.01)\n",
    "# Use categorical_crossentropy as loss function, and accuracy as one of the metrics (you may add other metrics too). \n",
    "# use model.compile to add the above parameters to the model\n",
    "model3.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Use model.fit to train the model. Following hyperparameters are suggested (but you may change them if you want):\n",
    "# batch_size = 32\n",
    "# epochs = 3\n",
    "# Validation data percentage: 20%\n",
    "model3.fit(x = x_train, y= y_train, batch_size=32, epochs=3,\n",
    "        validation_split=0.2, shuffle=True)\n",
    "\n",
    "model3.evaluate(x=x_test, y=y_test, batch_size=None, verbose=0, sample_weight=None, steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Not enough hidden layers and neurons!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
